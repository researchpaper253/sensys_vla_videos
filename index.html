<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Failure Case GIFs – VLA Models for Robotic Manipulation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="Supplementary failure-case GIFs for the paper 'How Aligned Are Vision–Language–Action Models in Robotics? An Empirical Study in Real and Simulated Environments'."
  />
  <style>
    :root {
      --bg: #f5f5f7;
      --card-bg: #ffffff;
      --border: #e0e0e5;
      --text: #111827;
      --muted: #6b7280;
      --accent: #2563eb;
      --radius-lg: 14px;
    }

    *, *::before, *::after {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont,
        "Segoe UI", sans-serif;
      color: var(--text);
      background: var(--bg);
      -webkit-font-smoothing: antialiased;
    }

    a {
      color: var(--accent);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .page {
      max-width: 960px;
      margin: 0 auto;
      padding: 24px 16px 40px;
    }

    header.hero {
      margin-bottom: 32px;
      padding: 20px 18px 22px;
      background: #ffffff;
      border-radius: 18px;
      border: 1px solid var(--border);
      box-shadow: 0 10px 30px rgba(15, 23, 42, 0.06);
    }

    .tagline {
      font-size: 12px;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--muted);
      margin-bottom: 6px;
    }

    h1 {
      font-size: clamp(24px, 3vw, 30px);
      line-height: 1.2;
      margin: 0 0 8px;
    }

    .subtitle {
      max-width: 720px;
      font-size: 14px;
      color: var(--muted);
      line-height: 1.5;
      margin-bottom: 6px;
    }

    .meta {
      font-size: 12px;
      color: var(--muted);
    }

    main {
      margin-top: 10px;
    }

    section {
      margin-top: 28px;
    }

    section:first-of-type {
      margin-top: 0;
    }

    h2 {
      font-size: 17px;
      margin: 0 0 6px;
    }

    .section-note {
      font-size: 13px;
      color: var(--muted);
      margin-bottom: 12px;
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 16px;
    }

    figure.clip {
      margin: 0;
      background: var(--card-bg);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border);
      overflow: hidden;
      display: flex;
      flex-direction: column;
    }

    figure.clip img {
      width: 100%;
      display: block;
      background: #0f172a;
    }

    figcaption {
      padding: 10px 12px 11px;
      font-size: 13px;
    }

    .clip-title {
      font-weight: 600;
      margin-bottom: 4px;
    }

    .clip-meta {
      font-size: 11px;
      color: var(--muted);
      margin-bottom: 4px;
    }

    .clip-desc {
      font-size: 12px;
      color: var(--muted);
      margin: 0;
    }

    .small {
      font-size: 12px;
      color: var(--muted);
      line-height: 1.5;
    }

    footer {
      margin-top: 32px;
      padding-top: 12px;
      border-top: 1px solid var(--border);
      font-size: 11px;
      color: var(--muted);
      text-align: center;
    }

    @media (max-width: 640px) {
      header.hero {
        padding: 16px 14px 18px;
      }
    }
  </style>
</head>
<body>
  <div class="page">
    <header class="hero">
      <div class="tagline">Supplementary Artifact · Failure Case GIFs</div>
      <h1>Experiences from Benchmarking Vision–Language–Action Models for Robotic Manipulation</h1>
      <p class="subtitle">
        This page provides a compact gallery of representative
        <strong>failure cases</strong> for ACT, OpenVLA–OFT, RDT-1B, and
        &pi;<sub>0</sub> across real-world ALOHA Mobile experiments and a
        simple cube-manipulation simulator.
      </p>
      <p class="meta">Anonymous submission · SenSys&nbsp;’26 · Please do not share publicly during review.</p>
    </header>

    <main>
      <!-- Short overview -->
      <section id="overview">
        <h2>Overview</h2>
        <p class="small">
          Each GIF corresponds to a single trial from our benchmark and is
          labeled by <strong>task</strong>, <strong>model</strong>, and
          <strong>dominant failure category</strong> (e.g., pre-grasp/grasp,
          release/placement, trajectory drift, instruction adherence). These
          visual examples complement the quantitative results and taxonomy
          reported in the paper.
        </p>
      </section>

      <!-- Real-world section -->
      <section id="real">
        <h2>Real-World Failure Cases (ALOHA Mobile)</h2>
        <p class="section-note">
          Four household-inspired tasks under ID / Spatial OOD / Instance+Spatial OOD settings.
        </p>

        <div class="grid">
          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/clean_dish_mis_grasp_release.gif"
              alt="Clean Dish clip showing the sponge slipping away before it can be deposited in the pot."
            />
            <figcaption>
              <div class="clip-title">Clean Dish – mis-grasp and premature release</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Clean Dish · Failure: Pre-grasp + Release
              </div>
              <p class="clip-desc">
                A slight offset causes the sponge to wobble in the gripper and fall
                before the pot is reached, highlighting how compounding small pose
                errors doom long-horizon interactions.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/accident_release_pot.gif"
              alt="Put Sponge into Pot clip where the grasped sponge is dropped against the rim."
            />
            <figcaption>
              <div class="clip-title">Put Sponge into Pot – accidental rim release</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Put Sponge into Pot · Failure: Release / Placement
              </div>
              <p class="clip-desc">
                The controller reaches the pot but opens slightly early, causing the
                sponge to catch the rim and tumble out before landing inside.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/misgrasp_pot_1.gif"
              alt="Put Sponge into Pot clip where the gripper repeatedly clips the pot wall."
            />
            <figcaption>
              <div class="clip-title">Put Sponge into Pot – repeated wall collisions</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Put Sponge into Pot · Failure: Trajectory drift
              </div>
              <p class="clip-desc">
                Small pose drift accumulates until the end-effector drags along the
                pot wall instead of approaching over the opening, preventing a clean
                insertion.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/misgrasp_pot_2.gif"
              alt="Put Sponge into Pot clip where the gripper closes on air next to the sponge."
            />
            <figcaption>
              <div class="clip-title">Put Sponge into Pot – air grasp near the sponge</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Put Sponge into Pot · Failure: Pre-grasp / Grasp
              </div>
              <p class="clip-desc">
                The gripper follows the right macro motion but closes a few
                centimeters away from the sponge, showcasing inconsistent contact
                reasoning under occlusion.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/misgrasp_zipper.gif"
              alt="Unzip Bag clip where the gripper closes on fabric instead of the zipper tab."
            />
            <figcaption>
              <div class="clip-title">Unzip Bag – zipper tab miss</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Unzip Bag · Failure: Fine-feature grasp
              </div>
              <p class="clip-desc">
                Despite approaching the zipper head, the fingers pinch nearby fabric
                and fail to latch onto the metal pull, so the zipper never actuates.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/fold_short_state_drift.gif"
              alt="Folding Shorts clip where the cloth slowly drifts out of the camera view."
            />
            <figcaption>
              <div class="clip-title">Fold Shorts – state drift mid-task</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Fold Shorts · Failure: State tracking / Drift
              </div>
              <p class="clip-desc">
                After the first fold, the cloth shifts relative to the gripper
                reference, so the next motion misses the target edge and the garment
                unfolds.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/misgrasp_fold_shorts.gif"
              alt="Folding Shorts clip where the grasp scoops only a corner of the garment."
            />
            <figcaption>
              <div class="clip-title">Fold Shorts – partial grasp on the hem</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Fold Shorts · Failure: Pre-grasp / Grasp
              </div>
              <p class="clip-desc">
                Only a small corner is captured, causing the cloth to twist and pull
                away from the desired fold line when lifted.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/real/Instruction_adherence.gif"
              alt="Clip where the robot picks the right object but ignores the spoken instruction."
            />
            <figcaption>
              <div class="clip-title">Instruction adherence lapse</div>
              <div class="clip-meta">
                Setting: Real robot · Task: Language-guided manipulation · Failure: Instruction adherence
              </div>
              <p class="clip-desc">
                The policy grounds the scene but executes a default clean-up motion
                instead of the specified command, underscoring alignment gaps between
                language and action.
              </p>
            </figcaption>
          </figure>
        </div>
      </section>

      <!-- Simulation section -->
      <section id="sim">
        <h2>Simulation Failure Cases (Cube Manipulation)</h2>
        <p class="section-note">
          Simple pick-and-place and stacking tasks with colored cubes, evaluated under ID / OOD conditions.
        </p>

        <div class="grid">
          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/sim/openvla_misgrap_red_cube.gif"
              alt="Simulation clip where the gripper keeps closing beside the red cube."
            />
            <figcaption>
              <div class="clip-title">Pick Red Cube – planar mis-grasp</div>
              <div class="clip-meta">
                Setting: Simulation · Task: Pick Red Cube · Failure: Pre-grasp / Grasp
              </div>
              <p class="clip-desc">
                The wrist follows the right macro trajectory but never centers over
                the cube, so every attempt ends with an air grasp just a few
                millimeters away.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/sim/sim_push_instead_pick.gif"
              alt="Simulation clip where the end-effector pushes the cube across the table."
            />
            <figcaption>
              <div class="clip-title">Pick Blue Cube – push instead of pinch</div>
              <div class="clip-meta">
                Setting: Simulation · Task: Pick Blue Cube · Failure: Contact mode error
              </div>
              <p class="clip-desc">
                The approach yaw is misaligned, so the gripper nudges the cube rather
                than settling above it, pushing it farther away on every attempt.
              </p>
            </figcaption>
          </figure>

          <figure class="clip">
            <img
              loading="lazy"
              src="gifs/sim/pi0_stack_blue_on_green.gif"
              alt="Simulation clip where a blue cube slides off the green base during stacking."
            />
            <figcaption>
              <div class="clip-title">Stack Blue on Green – release misalignment</div>
              <div class="clip-meta">
                Setting: Simulation · Task: Stack Blue on Green · Failure: Release / Placement
              </div>
              <p class="clip-desc">
                The second cube is transported correctly but released with a slight
                roll error, so it slides off the target stack and topples.
              </p>
            </figcaption>
          </figure>
        </div>
      </section>

      <!-- How to interpret / cite -->
      <section id="usage">
        <h2>Using and Citing This Artifact</h2>
        <p class="small">
          These GIFs are intended as qualitative complements to the figures and
          tables in the main paper (real-world and simulation results, failure
          taxonomy, and remedy discussion). If you reference this artifact,
          please cite the paper:
        </p>
        <p class="small">
          <em>
            “How Aligned Are Vision–Language–Action Models in Robotics?
            An Empirical Study in Real and Simulated Environments”, SenSys ’26.
          </em>
        </p>
      </section>
    </main>

    <footer>
      Hosted on GitHub Pages · This page is anonymized for peer review.
    </footer>
  </div>
</body>
</html>
